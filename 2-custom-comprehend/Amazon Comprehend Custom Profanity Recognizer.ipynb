{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING\n",
    "*By design, the training and test data in this notebook is hurtful, offensive, and hateful.*\n",
    "\n",
    "*The authors are grateful for the datasets made available by others to permit this kind of exploration, but are aware that some of this content is hurtful while other content may seem benign.  Considering some of these terms as potentially unsafe may be separately hurtful as well.*\n",
    "\n",
    "*The goal here is to offer a framework for exploration using AWS Services, not to offer opinions on the content.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "For moderation of user generated text content online, customers often ask about tools available to flag potentially offensive content.  While every community is ultimately responsible for its content standards, there are some general guidelines that community stewards tend to use to find the balance between protecting free speech online and guarding against speech that primarily insults, offends, or incites violence or hatred against individuals or groups of individuals.\n",
    "\n",
    "In practice, content moderation requires a lot more than identifying potentially offensive content; we share these links to remind you that this lab is not a substitute for a considerate content moderation program, but the techniques discussed here can be part of such a program.\n",
    "* [Content Moderation is Broken. Let Us Count the Ways](https://www.eff.org/deeplinks/2019/04/content-moderation-broken-let-us-count-ways), a provocative post by the Electronic Frontier Foundation\n",
    "* [The people policing the internet's most horrific content](https://www.bbc.com/news/business-49393858), a BBC News story on the working conditions for content moderators\n",
    "\n",
    "In research, profanity detection is considered distinct from hate speech and offensive language that may be directed toward individuals (or groups of individuals).  Take a look at the [SemEval-2019 tasks](http://alt.qcri.org/semeval2019/index.php?id=tasks) to get a sense of the language used to describe these efforts.\n",
    "\n",
    "For our lab today, we are going to focus on the narrow task of identifying profanity in tweets with a hope that participants learn how to use similar techniques to address specific needs.  Specifically, we're going to build an [Amazon Comprehend Custom Entity Recognizer](https://docs.aws.amazon.com/comprehend/latest/dg/custom-entity-recognition.html) to help us detect profanity in a curated collection of tweets.\n",
    "\n",
    "### Datasets\n",
    "* For a list of terms considered profane (our \"entity list\"), we used \"a list of 1,300+ English terms that could be found offensive\" made available by [Luis von Ahn's Research Group](https://www.cs.cmu.edu/~biglou/resources/).\n",
    "* For training and evaluation documents, we used a human-labeled tweet [dataset](https://github.com/t-davidson/hate-speech-and-offensive-language) shared by the authors of [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf), an investigation into lexical detection methods for the separation of hate speech from other offensive language.  \n",
    "As expected, this dataset provides additional metadata to help us see what's considered hate speech or generally offensive.\n",
    "  * ``count``:  number of CrowdFlower users who coded each tweet (minimum of 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CrowdFlower).\n",
    "  * ``hate_speech``:  number of CF users who judged the tweet to be hate speech.\n",
    "  * ``offensive_language``:  number of CF users who judged the tweet to be offensive.\n",
    "  * ``neither``:  number of CF users who judged the tweet to be neither offensive nor hate speech.\n",
    "  * ``class``:  class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS IAM Resources used in this notebook\n",
    "In general, this notebook does not use the AWS API to create or manipulate IAM resources; instructions are provided instead.\n",
    "\n",
    "The notebook code itself runs under a \"SageMaker execution role\" and has limited access, so participants will need to grant additional rights to be able to use Comprehend Custom.  In addition, Comprehend itself does not automatically have access to the Amazon S3 bucket used for data storage, so another IAM role (\"profanity-lab-role\") is used to grant access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's start off by getting to know our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "iamRole = get_execution_role()\n",
    "\n",
    "print('SageMaker execution role')\n",
    "print(iamRole) \n",
    "print('')\n",
    "print('This is the role that SageMaker would use to leverage AWS resources')\n",
    "print('(S3, CloudWatch) on your behalf.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create an Amazon S3 bucket where we can upload input/output files.\n",
    "\n",
    "Comprehend jobs will only read/write to a bucket in the same AWS region where the job is run so we use ``us-east-1`` throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "aws_region='us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a client we can use to interact with S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(service_name='s3', region_name=aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where should Comprehend read/write files?  To the lab bucket, of course.\n",
    "\n",
    "And where might we find the name for this bucket?  Check the outputs for the [AWS CloudFormation](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks?filteringText=&filteringStatus=active&viewNested=true&hideStacks=false) stack for this lab.  We need the value for the ``DataBucket`` key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=_GET_THIS_FROM_CLOUDFORMATION_STACK_OUTPUT_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that IAM role ARN we said we would need to pass around so Comprehend can read our data?  \n",
    "\n",
    "We need the value for the ``ComprehendDataAccessRoleARN`` key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_data_access_role_arn = _GET_THIS_FROM_CLOUDFORMATION_STACK_OUTPUT_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "\n",
    "To create a new Custom Entity Recognizer, we must provide training data; Comprehend will use most of this data to train the model and keep some of it aside to evaluate the model. \n",
    "\n",
    "Let's start by uploading training data to our bucket.  Specifically, we upload:\n",
    "1. a file that contains a list of values that we consider profane; and,\n",
    "1. a file with examples (documents) that contain these entities.\n",
    "\n",
    "*As the [documentation](https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html) explains, though Entity Lists are often easier to use, annotated document examples yield better results.  In real life, annotated examples are seldom available, so you might start with an Entity List first and use the first few classification results to get to Annotations.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our entity list look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -e Text -e crap profanity-list.csv | head -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do our annotated examples look like?  Remember that each line is considered a document here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -i -e ^tweet -e \"aspiring crazy\" profanity-training-docs-1k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload our files.  \n",
    "We need to provide a [minimum number of examples](https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html#limits-custom-entity-recognition) for training to succeed.  More examples will help with accuracy, but we don't have time for a 40-minute training cycle, so we start with a small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Filename='profanity-list.csv', \n",
    "    Bucket=bucket_name, \n",
    "    Key='private/labs/training/profanity-list.csv')\n",
    "\n",
    "s3.upload_file(\n",
    "    Filename='profanity-training-docs-1k.txt',\n",
    "    # Filename='profanity-training-docs-2k.txt',\n",
    "    Bucket=bucket_name, \n",
    "    Key='private/labs/training/profanity-training-docs.txt')\n",
    "\n",
    "s3.list_objects(Bucket=bucket_name, Prefix='private/labs/training')['Contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that our training data is in place, let's try to create a custom entity recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(service_name='comprehend', region_name=aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=comprehend.create_entity_recognizer(\n",
    "    RecognizerName='profanity',\n",
    "    DataAccessRoleArn=comprehend_data_access_role_arn,\n",
    "    InputDataConfig={\n",
    "        'EntityTypes': [ { \n",
    "            'Type': 'PROFANITY' }],\n",
    "        'EntityList': { \n",
    "            'S3Uri': 's3://' + bucket_name + '/private/labs/training/profanity-list.csv' },\n",
    "        'Documents': { \n",
    "            'S3Uri': 's3://' + bucket_name + '/private/labs/training/profanity-training-docs.txt' }\n",
    "    },\n",
    "    LanguageCode='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the training job complete?  (It should take about 20 minutes to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=result['EntityRecognizerArn'])['EntityRecognizerProperties']['Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for it to finish..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "status = comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=result['EntityRecognizerArn'])['EntityRecognizerProperties']['Status']\n",
    "while (status in ['SUBMITTED', 'TRAINING']):\n",
    "    status = comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn=result['EntityRecognizerArn'])['EntityRecognizerProperties']['Status']\n",
    "    print('.', end='')\n",
    "    time.sleep(60)\n",
    "\n",
    "print('')\n",
    "print(json.dumps(comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=result['EntityRecognizerArn'])['EntityRecognizerProperties']['RecognizerMetadata'],\n",
    "     indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are we done yet?\n",
    "\n",
    "While you're waiting, check out this blog post to learn how to [Build a custom entity recognizer using Amazon Comprehend](https://aws.amazon.com/blogs/machine-learning/build-a-custom-entity-recognizer-using-amazon-comprehend/).  (Look Ma -- no notebooks!)\n",
    "\n",
    "Or, check out some of the other [entity recognition related offerings on the AWS Marketplace](https://aws.amazon.com/marketplace/search/results?x=0&y=0&searchTerms=Entity+Recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should the training job fail (status is ``IN_ERROR``), please correct the issue, delete the recognizer, and try again.\n",
    "\n",
    "```\n",
    "comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=result['EntityRecognizerArn'])['EntityRecognizerProperties']['Message']\n",
    "comprehend.delete_entity_recognizer(EntityRecognizerArn=result['EntityRecognizerArn'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?  Do we have a reasonable recognizer here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(\n",
    "    comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn=result['EntityRecognizerArn'])\n",
    "    ['EntityRecognizerProperties']['RecognizerMetadata']['EntityTypes'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, we would like an F1Score greater than 80 with precision and recall to match.  For more information on these metrics and other suggestions to improve accuracy, check out the [documentation](https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have an entity recognizer, let's try it out with a test dataset.\n",
    "\n",
    "First, let's create a small evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the hate speech dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 profanity-test-docs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the quotes and newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('profanity-test-docs.txt', 'w+') as outfile:\n",
    "    with open('profanity-test-docs.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, quotechar='\"')\n",
    "        for row in reader:\n",
    "            if (row[-1] != 'tweet'):\n",
    "                outfile.write(row[-1].replace('\\n', ' ').replace('\"', ''))\n",
    "                outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l profanity-test-docs.*\n",
    "\n",
    "# line counts may not match up because the source file may have embedded newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's upload our test data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Filename='profanity-test-docs.txt', \n",
    "    Bucket=bucket_name, \n",
    "    Key='private/labs/test/profanity-test-docs.txt')\n",
    "\n",
    "s3.list_objects(Bucket=bucket_name, Prefix='private/labs/test/profanity-test-docs.txt')['Contents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start an entity recognition job with this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_result = comprehend.start_entities_detection_job(\n",
    "    JobName='profanity-2',\n",
    "    EntityRecognizerArn=result['EntityRecognizerArn'],\n",
    "    InputDataConfig={\n",
    "        'S3Uri': 's3://' + bucket_name + '/private/labs/test/profanity-test-docs.txt',\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE' },\n",
    "    OutputDataConfig={ 'S3Uri': 's3://' + bucket_name + '/private/labs/test/profanity-1/' },\n",
    "    DataAccessRoleArn=comprehend_data_access_role_arn,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "print(json.dumps(job_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for the job to finish.  This should take 7-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = comprehend.describe_entities_detection_job(\n",
    "    JobId=job_result['JobId'])['EntitiesDetectionJobProperties']['JobStatus']\n",
    "\n",
    "while (status == 'IN_PROGRESS'):\n",
    "    status = comprehend.describe_entities_detection_job(\n",
    "        JobId=job_result['JobId'])['EntitiesDetectionJobProperties']['JobStatus']\n",
    "    print('.', end='')\n",
    "    time.sleep(60)\n",
    "\n",
    "print('')\n",
    "print(comprehend.describe_entities_detection_job(JobId=job_result['JobId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are we done yet??\n",
    "\n",
    "While you're waiting, consider how you might include humans in the loop to confirm what the machine thinks.  Check out this blog post on [Verifying and adjusting your data labels to create higher quality training datasets with Amazon SageMaker Ground Truth](https://aws.amazon.com/blogs/machine-learning/verifying-and-adjusting-your-data-labels-to-create-higher-quality-training-datasets-with-amazon-sagemaker-ground-truth/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the output from S3 so we can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3URI = comprehend.describe_entities_detection_job(\n",
    "    JobId=job_result['JobId'])['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "p = urlparse(s3URI, allow_fragments=False)\n",
    "\n",
    "print('Downloading {}'.format(s3URI))\n",
    "s3.download_file(Bucket=p.netloc, Key=p.path.lstrip('/'), Filename='profanity_output.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains an output file in the [JSON Lines format](http://jsonlines.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvzf profanity_output.tar.gz && mv -v output profanity_output.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 2 profanity_output.jsonl\n",
    "!tail -n 2 profanity-test-docs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the output from Comprehend with what we expect.  First, let's create a dataframe with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "results_df = pd.read_json(path_or_buf='profanity_output.jsonl', lines=True)\n",
    "\n",
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also load up our test data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"profanity-test-docs.csv\", \n",
    "    quotechar='\"', quoting=csv.QUOTE_MINIMAL, escapechar='\\\\', engine='python', \n",
    "    warn_bad_lines=True)\n",
    "\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge our datasets so we can compare results with expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_df = pd.merge(\n",
    "    left=test_df, right=results_df, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can filter our merged dataset in different ways to see if we detect profanity as expected.  As a reminder, the counts represent the number of users who found the content to be hateful or offensive.\n",
    "\n",
    "This is a particularly hard dataset, so we limit our explorations, but you can uncomment the source below for yourself.  \n",
    "\n",
    "Try these out:\n",
    "* Can you find entries using a \"contains\" match?\n",
    "* What about terms we missed?  Terms that should have been considered profane, but weren't.  (False negatives)\n",
    "* Find any entries that were incorrectly classified as profane?  (False positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    #display(merged_results_df.drop(columns=['id', 'neither', 'File', 'Line']).head(25))\n",
    "    #display(merged_results_df[merged_results_df['tweet'].str.contains('stf')].drop(\n",
    "    #    columns=['id', 'neither', 'File', 'Line']).head(3))\n",
    "    display(merged_results_df[\n",
    "        (merged_results_df['class'] == 1) &\n",
    "        (merged_results_df['offensive_language'] > 2) & \n",
    "        (merged_results_df['Entities'].str.len() == 0)].drop(\n",
    "        columns=['id', 'neither', 'File', 'Line']).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, it would be interesting to create a classifier with the 2k dataset to see if it improves any of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "As you can see from the examples above, our profanity detector works well with unambiguous examples, but could use more work with additional terms, acronyms, deliberate obfuscation, and contextual resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further explorations\n",
    "* Though entity lists offer an easy way to get started with custom entity recognizers, [https://docs.aws.amazon.com/comprehend/latest/dg/cer-annotation.html](annotations) do a better job of treating terms in context (e.g., \"gung ho\") and often yield better results.  With this in mind, it would be interesting to review the output from this first entity recognizer to create an annotated data set for a more sophisticated recognizer.\n",
    "* It might be extremely useful to build a [Custom Classifier](https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html) to detect hate speech.\n",
    "* To go deeper, consider [Sentia Labs' AWS SageMaker post](https://www.sentialabs.io/2019/01/30/SageMaker-In-Action.html) on text classification using the BlazingText algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
